#!/usr/bin/env python3
"""
Extract YouTube video transcripts and metadata to Markdown format.
Saves to ~/Brains/brain/ with YAML frontmatter.

Now uses youtube-transcript-api for more reliable transcript extraction.
"""

import json
import subprocess
import sys
import re
from pathlib import Path
from datetime import datetime


def sanitize_filename(title):
    """Convert video title to safe filename."""
    # Remove invalid characters
    clean = re.sub(r'[<>:"/\\|?*]', '', title)
    # Replace spaces and limit length
    clean = clean.replace(' ', '_')
    return clean[:100]


def format_duration(seconds):
    """Convert seconds to HH:MM:SS format."""
    if not seconds:
        return "00:00:00"
    h = int(seconds // 3600)
    m = int((seconds % 3600) // 60)
    s = int(seconds % 60)
    return f"{h:02d}:{m:02d}:{s:02d}"


def format_timestamp(seconds):
    """Convert seconds to HH:MM:SS timestamp."""
    return format_duration(seconds)


def extract_metadata(url):
    """Extract video metadata using yt-dlp."""
    cmd = [
        'yt-dlp',
        '--dump-json',
        '--no-download',
        url
    ]

    result = subprocess.run(cmd, capture_output=True, text=True)
    if result.returncode != 0:
        raise Exception(f"Failed to extract metadata: {result.stderr}")

    return json.loads(result.stdout)


def extract_subtitles(video_id, languages=['ko', 'en', 'ja']):
    """Extract subtitles/transcript using youtube-transcript-api.

    Args:
        video_id: YouTube video ID
        languages: List of language codes to try (default: ko, en, ja)

    Returns:
        tuple: (transcript_entries, language_code) or (None, None) if failed
    """
    try:
        from youtube_transcript_api import YouTubeTranscriptApi

        api = YouTubeTranscriptApi()

        # Try each language in order
        for lang in languages:
            try:
                result = api.fetch(video_id, languages=[lang])

                # Convert to list of dictionaries with start, text, duration
                entries = []
                for entry in result:
                    start = entry.start if hasattr(entry, 'start') else entry.get('start', 0)
                    text = entry.text if hasattr(entry, 'text') else entry.get('text', '')
                    duration = entry.duration if hasattr(entry, 'duration') else entry.get('duration', 0)

                    # Format timestamp as HH:MM:SS
                    h = int(start // 3600)
                    m = int((start % 3600) // 60)
                    s = int(start % 60)
                    timestamp = f"{h:02d}:{m:02d}:{s:02d}"

                    entries.append({
                        'timestamp': timestamp,
                        'start': start,
                        'text': text,
                        'duration': duration
                    })

                if entries:
                    return entries, lang

            except Exception:
                # Try next language
                continue

        return None, None

    except ImportError:
        print("Error: youtube-transcript-api not installed.")
        print("Install with: pip install youtube-transcript-api")
        return None, None
    except Exception as e:
        print(f"Error extracting transcript: {e}")
        return None, None


def deduplicate_entries(entries):
    """Remove entries that are prefixes of subsequent entries."""
    if not entries:
        return []

    deduplicated = []

    for idx in range(len(entries)):
        timestamp = entries[idx]['timestamp']
        text = entries[idx]['text']

        # Check if this text is a prefix of any subsequent entry
        is_prefix = False
        for next_idx in range(idx + 1, min(idx + 5, len(entries))):
            next_text = entries[next_idx]['text']

            if next_text.startswith(text) and len(next_text) > len(text):
                is_prefix = True
                break

        if not is_prefix:
            deduplicated.append(entries[idx])

    return deduplicated


def group_by_chapters(transcript_entries, chapters):
    """Group transcript entries by video chapters."""
    if not chapters:
        return [("Transcript", transcript_entries)]

    grouped = []
    chapter_times = [(ch['start_time'], ch['title']) for ch in chapters]
    chapter_times.append((float('inf'), None))  # End marker

    for i, (start_time, title) in enumerate(chapter_times[:-1]):
        next_start = chapter_times[i + 1][0]

        # Filter transcript entries for this chapter
        chapter_entries = []
        for entry in transcript_entries:
            total_seconds = entry['start']

            if start_time <= total_seconds < next_start:
                chapter_entries.append(entry)

        if chapter_entries:
            grouped.append((title, chapter_entries))

    return grouped if grouped else [("Transcript", transcript_entries)]


def create_markdown(metadata, transcript_entries):
    """Create Markdown document with YAML frontmatter."""
    # Extract metadata fields
    title = metadata.get('title', 'Unknown')
    channel = metadata.get('channel', metadata.get('uploader', 'Unknown'))
    url = metadata.get('webpage_url', '')
    upload_date = metadata.get('upload_date', '')
    if upload_date:
        upload_date = f"{upload_date[:4]}-{upload_date[4:6]}-{upload_date[6:]}"
    duration = format_duration(metadata.get('duration'))
    description = metadata.get('description', '').replace('\n', ' ').strip()
    tags = metadata.get('tags', [])
    view_count = metadata.get('view_count', 0)
    like_count = metadata.get('like_count', 0)
    chapters = metadata.get('chapters', [])

    # Build YAML frontmatter
    md = "---\n"
    md += f"title: \"{title}\"\n"
    md += f"channel: \"{channel}\"\n"
    md += f"url: {url}\n"
    md += f"upload_date: {upload_date}\n"
    md += f"duration: {duration}\n"
    md += f"description: \"{description[:500]}...\"\n"
    md += f"tags: {json.dumps(tags)}\n"
    md += f"view_count: {view_count}\n"
    md += f"like_count: {like_count}\n"
    md += "---\n\n"

    # Add title
    md += f"# {title}\n\n"

    # Group transcript by chapters
    grouped = group_by_chapters(transcript_entries, chapters)

    for chapter_title, entries in grouped:
        md += f"## {chapter_title}\n\n"

        for entry in entries:
            timestamp = entry['timestamp']
            text = entry['text']
            md += f"**{timestamp}** {text}\n\n"

    return md


def main():
    if len(sys.argv) < 2:
        print("Usage: python extract_transcript.py <youtube_url> [output_filename]")
        sys.exit(1)

    url = sys.argv[1]
    custom_filename = sys.argv[2] if len(sys.argv) > 2 else None

    print(f"Extracting metadata from {url}...")
    metadata = extract_metadata(url)

    video_id = metadata['id']
    title = metadata.get('title', 'Unknown')

    print(f"Extracting subtitles for: {title}")

    # Extract transcript using youtube-transcript-api
    transcript_entries, lang = extract_subtitles(video_id)

    if not transcript_entries:
        print("No subtitles available for this video.")
        print("Tried languages: ko (Korean), en (English), ja (Japanese)")
        sys.exit(1)

    print(f"Found {lang} subtitles ({len(transcript_entries)} entries)")

    print("Removing duplicates...")
    transcript_entries = deduplicate_entries(transcript_entries)
    print(f"After deduplication: {len(transcript_entries)} entries")

    print("Creating Markdown document...")
    markdown = create_markdown(metadata, transcript_entries)

    # Determine output path
    vault_path = Path.home() / "Brains" / "brain"
    if custom_filename:
        output_file = vault_path / custom_filename
    else:
        filename = sanitize_filename(title) + ".md"
        output_file = vault_path / filename

    # Ensure vault directory exists
    vault_path.mkdir(parents=True, exist_ok=True)

    # Write output
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(markdown)

    print(f"âœ“ Saved to: {output_file}")


if __name__ == '__main__':
    main()
